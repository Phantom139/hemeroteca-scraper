{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read input file from D:/Robert Docs/College/NIU/PhD/SP18_Research\n",
      "Parsing sheet 1\n",
      "Saving: aguardente_1860_alagoas => 697.749065894\n",
      "Saving: aguardente_1861_alagoas => 413.978206023\n",
      "Saving: aguardente_1862_alagoas => 387.642276423\n",
      "Saving: cotton_1860_alagoas => 6618.43404544\n",
      "Saving: cotton_1861_alagoas => 8284.84700828\n",
      "Saving: cotton_1862_alagoas => 17460.1133445\n",
      "Saving: sugar_1860_alagoas => 2469.35140994\n",
      "Saving: sugar_1861_alagoas => 2213.93581515\n",
      "Saving: sugar_1862_alagoas => 2444.41207289\n",
      "Saving: cooking-oil_1860_alagoas => nan\n",
      "Saving: cooking-oil_1861_alagoas => 1098.31438652\n",
      "Saving: cooking-oil_1862_alagoas => 1451.49700599\n",
      "Saving: rice_1860_alagoas => 3462.99212598\n",
      "Saving: rice_1861_alagoas => 1710.16393443\n",
      "Saving: rice_1862_alagoas => nan\n",
      "Saving: crackers_1860_alagoas => nan\n",
      "Saving: crackers_1861_alagoas => nan\n",
      "Saving: crackers_1862_alagoas => 20000\n",
      "Saving: potatoes_1860_alagoas => nan\n",
      "Saving: potatoes_1861_alagoas => 160000\n",
      "Saving: potatoes_1862_alagoas => 4228.34645669\n",
      "Saving: coffee_1860_alagoas => 6400\n",
      "Saving: coffee_1861_alagoas => nan\n",
      "Saving: coffee_1862_alagoas => 3904.76190476\n",
      "Saving: jerked-beef_1860_alagoas => nan\n",
      "Saving: jerked-beef_1861_alagoas => 3484.21052632\n",
      "Saving: jerked-beef_1862_alagoas => 3000\n",
      "Saving: tea_1860_alagoas => nan\n",
      "Saving: tea_1861_alagoas => 2000\n",
      "Saving: tea_1862_alagoas => 2000\n",
      "Saving: wax_1860_alagoas => nan\n",
      "Saving: wax_1861_alagoas => 743.438864629\n",
      "Saving: wax_1862_alagoas => 1000\n",
      "Saving: cigars_1860_alagoas => 29.7142857143\n",
      "Saving: cigars_1861_alagoas => 294.810539761\n",
      "Saving: cigars_1862_alagoas => 1468.86258363\n",
      "Saving: dried-leather-hides_1860_alagoas => 404.893964111\n",
      "Saving: dried-leather-hides_1861_alagoas => 34.5906432749\n",
      "Saving: dried-leather-hides_1862_alagoas => 30.2181818182\n",
      "Saving: salted-leather-hides_1860_alagoas => 5001.03444444\n",
      "Saving: salted-leather-hides_1861_alagoas => 4913.14285714\n",
      "Saving: salted-leather-hides_1862_alagoas => 7759.79112272\n",
      "Saving: rafts-and-canoes_1860_alagoas => 240\n",
      "Saving: rafts-and-canoes_1861_alagoas => 62500\n",
      "Saving: rafts-and-canoes_1862_alagoas => nan\n",
      "Saving: various-sweets_1860_alagoas => nan\n",
      "Saving: various-sweets_1861_alagoas => 700\n",
      "Saving: various-sweets_1862_alagoas => nan\n",
      "Saving: flour_1860_alagoas => 400\n",
      "Saving: flour_1861_alagoas => nan\n",
      "Saving: flour_1862_alagoas => 113.75\n",
      "Saving: leaf-tobacco_1860_alagoas => nan\n",
      "Saving: leaf-tobacco_1861_alagoas => 3843.23307494\n",
      "Saving: leaf-tobacco_1862_alagoas => 8000\n",
      "Saving: bran_1860_alagoas => 500\n",
      "Saving: bran_1861_alagoas => nan\n",
      "Saving: bran_1862_alagoas => nan\n",
      "Saving: beans_1860_alagoas => 1800\n",
      "Saving: beans_1861_alagoas => nan\n",
      "Saving: beans_1862_alagoas => nan\n",
      "Saving: hardware_1860_alagoas => nan\n",
      "Saving: hardware_1861_alagoas => nan\n",
      "Saving: hardware_1862_alagoas => nan\n",
      "Saving: chalk_1860_alagoas => nan\n",
      "Saving: chalk_1861_alagoas => nan\n",
      "Saving: chalk_1862_alagoas => 1411.76470588\n",
      "Saving: troughs_1860_alagoas => 1666.66666667\n",
      "Saving: troughs_1861_alagoas => nan\n",
      "Saving: troughs_1862_alagoas => nan\n",
      "Saving: rafts_1860_alagoas => 25000\n",
      "Saving: rafts_1861_alagoas => nan\n",
      "Saving: rafts_1862_alagoas => nan\n",
      "Saving: fibers-from-the-wool-tree-or-castanha-do-ceara-tree_1860_alagoas => nan\n",
      "Saving: fibers-from-the-wool-tree-or-castanha-do-ceara-tree_1861_alagoas => nan\n",
      "Saving: fibers-from-the-wool-tree-or-castanha-do-ceara-tree_1862_alagoas => 5745.45454545\n",
      "Saving: firewood_1860_alagoas => 252.19047619\n",
      "Saving: firewood_1861_alagoas => 5\n",
      "Saving: firewood_1862_alagoas => 12.8\n",
      "Saving: camomile_1860_alagoas => 2560\n",
      "Saving: camomile_1861_alagoas => nan\n",
      "Saving: camomile_1862_alagoas => 2712\n",
      "Saving: wood_1860_alagoas => 13190.2735918\n",
      "Saving: wood_1861_alagoas => 13471.6314199\n",
      "Saving: wood_1862_alagoas => 4359.15492958\n",
      "Saving: furniture_1860_alagoas => 351.133869788\n",
      "Saving: furniture_1861_alagoas => nan\n",
      "Saving: furniture_1862_alagoas => 100000\n",
      "Saving: corn_1860_alagoas => nan\n",
      "Saving: corn_1861_alagoas => nan\n",
      "Saving: corn_1862_alagoas => 2591.65384615\n",
      "Saving: castor-oil/oleo-de-momona_1860_alagoas => nan\n",
      "Saving: castor-oil/oleo-de-momona_1861_alagoas => 300\n",
      "Saving: castor-oil/oleo-de-momona_1862_alagoas => nan\n",
      "Saving: castor-oil/oleo-de-ricino_1860_alagoas => nan\n",
      "Saving: castor-oil/oleo-de-ricino_1861_alagoas => 581.224489796\n",
      "Saving: castor-oil/oleo-de-ricino_1862_alagoas => nan\n",
      "Saving: miscellaneous-oils_1860_alagoas => 178.365489458\n",
      "Saving: miscellaneous-oils_1861_alagoas => nan\n",
      "Saving: miscellaneous-oils_1862_alagoas => nan\n",
      "Saving: carnauba-palm-straw_1860_alagoas => nan\n",
      "Saving: carnauba-palm-straw_1861_alagoas => 583.005366726\n",
      "Saving: carnauba-palm-straw_1862_alagoas => nan\n",
      "Saving: salted-fish_1860_alagoas => 4000\n",
      "Saving: salted-fish_1861_alagoas => nan\n",
      "Saving: salted-fish_1862_alagoas => nan\n",
      "Saving: grinding-stones_1860_alagoas => nan\n",
      "Saving: grinding-stones_1861_alagoas => nan\n",
      "Saving: grinding-stones_1862_alagoas => 500\n",
      "Saving: palm-fiber_1860_alagoas => nan\n",
      "Saving: palm-fiber_1861_alagoas => nan\n",
      "Saving: palm-fiber_1862_alagoas => 500\n",
      "Saving: soap_1860_alagoas => nan\n",
      "Saving: soap_1861_alagoas => 100\n",
      "Saving: soap_1862_alagoas => 100\n",
      "Saving: leather-soles-(shoes/sandals)_1860_alagoas => 850.931677019\n",
      "Saving: leather-soles-(shoes/sandals)_1861_alagoas => nan\n",
      "Saving: leather-soles-(shoes/sandals)_1862_alagoas => nan\n",
      "Saving: tallow_1860_alagoas => nan\n",
      "Saving: tallow_1861_alagoas => 5662.07627119\n",
      "Saving: tallow_1862_alagoas => 2974.02597403\n",
      "Saving: seeds_1860_alagoas => nan\n",
      "Saving: seeds_1861_alagoas => nan\n",
      "Saving: seeds_1862_alagoas => 2000\n",
      "Saving: planks-of-laurel-wood_1860_alagoas => nan\n",
      "Saving: planks-of-laurel-wood_1861_alagoas => 35000\n",
      "Saving: planks-of-laurel-wood_1862_alagoas => 24000\n",
      "Saving: planks-of-yellowheart-wood_1860_alagoas => nan\n",
      "Saving: planks-of-yellowheart-wood_1861_alagoas => nan\n",
      "Saving: planks-of-yellowheart-wood_1862_alagoas => 5000\n",
      "Saving: planks-of-cedar-wood_1860_alagoas => nan\n",
      "Saving: planks-of-cedar-wood_1861_alagoas => nan\n",
      "Saving: planks-of-cedar-wood_1862_alagoas => 3000\n",
      "Saving: cashew-wine_1860_alagoas => nan\n",
      "Saving: cashew-wine_1861_alagoas => nan\n",
      "Saving: cashew-wine_1862_alagoas => 4000\n",
      "Saving: aguardente_1854_bahia => 0,774\n",
      "Saving: aguardente_1855_bahia => 0,892\n",
      "Saving: aguardente_1856_bahia => 1,123\n",
      "Saving: cotton-raw_1854_bahia => 5,765\n",
      "Saving: cotton-raw_1855_bahia => 5500\n",
      "Saving: cotton-raw_1856_bahia => 6,630\n",
      "Saving: sugar-white-good-quality_1854_bahia => 2,484\n",
      "Saving: sugar-white-good-quality_1855_bahia => 2,834\n",
      "Saving: sugar-white-good-quality_1856_bahia => 1,054\n",
      "Saving: sugar-white-ordinary_1854_bahia => 1,981\n",
      "Saving: sugar-white-ordinary_1855_bahia => 2,668\n",
      "Saving: sugar-white-ordinary_1856_bahia => 3,908\n",
      "Saving: sugar-brown_1854_bahia => 1,706\n",
      "Saving: sugar-brown_1855_bahia => 2,272\n",
      "Saving: sugar-brown_1856_bahia => 3,181\n",
      "Saving: coffee-dehulled_1854_bahia => 2,184\n",
      "Saving: coffee-dehulled_1855_bahia => 3,325\n",
      "Saving: coffee-dehulled_1856_bahia => 5,159\n",
      "Saving: cacao_1854_bahia => 2,184\n",
      "Saving: cacao_1855_bahia => 3,325\n",
      "Saving: cacao_1856_bahia => 5,159\n",
      "Saving: leather-dried_1854_bahia => 0,215\n",
      "Saving: leather-dried_1855_bahia => 0,253\n",
      "Saving: leather-dried_1856_bahia => 0,374\n",
      "Saving: leather-salted_1854_bahia => 0,167\n",
      "Saving: leather-salted_1855_bahia => 0,205\n",
      "Saving: leather-salted_1856_bahia => 0,328\n",
      "Saving: tobacco-leaf_1854_bahia => 3,282\n",
      "Saving: tobacco-leaf_1855_bahia => 3,769\n",
      "Saving: tobacco-leaf_1856_bahia => 5,835\n",
      "Saving: tobacco-rolled_1854_bahia => 3,066\n",
      "Saving: tobacco-rolled_1855_bahia => 2,700\n",
      "Saving: tobacco-rolled_1856_bahia => 5,373\n",
      "Saving: mangotes-de-gibraltar_1854_bahia => 11,829\n",
      "Saving: mangotes-de-gibraltar_1855_bahia => 12,125\n",
      "Saving: mangotes-de-gibraltar_1856_bahia => 2,2758\n",
      "Saving: meat-fish-and-misc-animal-products_1872_bahia => nan\n",
      "Saving: cereal-fruits-and-other-foodstuffs_1872_bahia => 122\n",
      "Saving: tobacco-and-tobacco-products_1872_bahia => 897\n",
      "Saving: molasses_1872_bahia => 98\n",
      "Saving: objects-for-natural-history_1872_bahia => 306\n",
      "Saving: coffee_1872_bahia => 501\n",
      "Saving: cacao_1872_bahia => 299\n",
      "Saving: leather-and-hides_1872_bahia => 685\n",
      "Saving: aguardente_1860_ceara => 1000\n",
      "Saving: cotton_1860_ceara => 7496\n",
      "Saving: brown-sugar_1860_ceara => 2000\n",
      "Saving: sugar-not-specified_1860_ceara => 2000\n",
      "Saving: rice_1860_ceara => 4000\n",
      "Saving: coffee_1860_ceara => 6000\n",
      "Saving: manioc-flour_1860_ceara => 2500\n",
      "Saving: beans_1860_ceara => 4000\n",
      "Saving: corn_1860_ceara => 2000\n",
      "Saving: castor-oil_1860_ceara => 3200\n",
      "Saving: polvilho/tapioca_1860_ceara => 800\n",
      "Writing New Table\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "DEFINE PROGRAM CONSTANTS\n",
    "\"\"\"\n",
    "# This is the file that is being read in\n",
    "inputFile = r\"Domestic X and M prices.xlsx\"\n",
    "\n",
    "# These are the two files that will be created / updated\n",
    "outFile1 = \"XFromProv.csv\"\n",
    "outFile2 = \"MToProv.csv\"\n",
    "\n",
    "# Start by assigning the current working directory\n",
    "currentDir = 'D:/Robert Docs/College/NIU/PhD/SP18_Research'\n",
    "\n",
    "# A list of all of the provinces we are interested in, only data from these provinces will be recorded\n",
    "#  in the output tables, anything else will be ignored.\n",
    "ProvinceList = [\"Alagoas\", \"Amazonas\", \"Bahia\", \"Ceara\", \"Espirito Santo\", \"Goias\", \"Maranhao\", \"Mato Grosso\", \"Minas Gerais\",\n",
    "                \"Para\", \"Paraiba\", \"Parana\", \"Pernambuco\", \"Piaui\", \"Rio De Janeiro\", \"Rio Grande do Norte\", \"Rio Grande do Sul\",\n",
    "\t\t\t\t\"Santa Catarina\", \"Sao Paulo\", \"Sergipe\"]\n",
    "\n",
    "# targetCol defines the column in each sub-block we're looking for, this can be modified in the future to map other\n",
    "#  quantities of interest. This is an integer totalizer field, so it counts how many columns over in each sub block to use\n",
    "targetName = [\"value per unit\", \"price per unit\", \"avg prices reported per year\"]\n",
    "\n",
    "# identifier defines what connects two rows together, signifying the beginning of a sub-block\n",
    "identifier = \"source link\"\n",
    "\n",
    "\"\"\"\n",
    "BEGIN CODE\n",
    "\"\"\"\n",
    "\n",
    "print(\"Attempting to read input file from \" + currentDir)\n",
    "\n",
    "xlsx = pd.ExcelFile(currentDir + '/' + inputFile)\n",
    "# Ref: Sheet 0: X from Prov to other provs, Note: As the first row read is considered a header, we only skip the first\n",
    "df1 = xlsx.parse(1, skiprows=0, header=None)\n",
    "# Ref: Sheet 1: M to Prov from other provs\n",
    "df2 = xlsx.parse(2, skiprows=0, header=None)\n",
    "\n",
    "print(\"Parsing sheet 1\")\n",
    "# Define Sheet 1 locals\n",
    "targetCol = 0\n",
    "saveDict = { }\n",
    "sheet1 = []\n",
    "colIndex = 0\n",
    "subBlocks = []\n",
    "geoSpatial = []\n",
    "for i, row in enumerate(df1.itertuples(), 0):\n",
    "    sheet1.append(tuple((i, row)))\n",
    "\n",
    "for (i, row) in sheet1:   \n",
    "    # Our first step will be to construct the \"sub-blocks\", so check for any \"named\" column, and then below it for the \n",
    "    #  identifier\n",
    "    rowSeries = pd.Series(row)\n",
    "    # The first two columns contain the geo-temporal information, but we need to make sure it's not \"subheaders\"\n",
    "    #  This is a simple digit test on the first four characters in the string of the \"year\" value\n",
    "    if(str(rowSeries[1])[0:4].isdigit()):\n",
    "        # Grab the second column as well\n",
    "        val = (str(rowSeries[1]).strip())[0:4]\n",
    "        # We now need to define the starting point of this block so the program can identify the link correctly.\n",
    "        #  To do this, we loop back up repeatidly until we find the block that has a string or is empty.\n",
    "        foundStart = False\n",
    "        currentIndex = i\n",
    "        linkRow = 0\n",
    "        while(foundStart == False):\n",
    "            prev = str(pd.Series(sheet1[currentIndex])[1][1])[0:4]\n",
    "            if(prev.isdigit() == False):\n",
    "                # Got it!\n",
    "                linkRow = currentIndex + 1\n",
    "                break\n",
    "            # Keep going...\n",
    "            currentIndex -= 1\n",
    "            if(currentIndex < 0):\n",
    "                print(\"ERROR: Cannot locate link\")\n",
    "                break\n",
    "        append = tuple((int(i), int(linkRow), val, rowSeries[2]))\n",
    "        geoSpatial.append(append)\n",
    "        \n",
    "    for col in rowSeries:\n",
    "        # Check for any open blocks with text and nans in both the spots to it's left and right\n",
    "        if(colIndex >= 1 and colIndex < rowSeries.size-1):\n",
    "            test = col\n",
    "            testPrev = rowSeries[colIndex - 1]\n",
    "            testNext = rowSeries[colIndex + 1]\n",
    "            if(testPrev == \"Destination if known\"):\n",
    "                testPrev = np.nan\n",
    "            if((pd.isnull(test) == False) and (pd.isnull(testPrev) and pd.isnull(testNext)) and (i < len(sheet1)-1)):\n",
    "                # It might be a column header we're looking for, peek at the next row and check for the link\n",
    "                nextRow = sheet1[i+1][1]\n",
    "                testCompare = str(nextRow[colIndex]).strip().lower()\n",
    "                if((testCompare == identifier) or (testCompare[0:5] == \"http:\") or (testCompare[0:6] == \"https:\")):\n",
    "                    # We have a match! Now find the column we're looking for\n",
    "                    for ix in range(colIndex, colIndex+4):\n",
    "                        if(ix < rowSeries.size - 1):\n",
    "                            for indTarget in targetName:\n",
    "                                if(indTarget in str(df1.iloc[:, ix]).lower()):\n",
    "                                    targetCol = ix+1\n",
    "                                    break\n",
    "                    if(str(sheet1[i+2][1][targetCol]).isdigit()):\n",
    "                        subBlocks.append(tuple((i+2, col, targetCol)))\n",
    "                    else:\n",
    "                        subBlocks.append(tuple((i+3, col, targetCol)))\n",
    "                    targetCol = 0\n",
    "        colIndex += 1\n",
    "    colIndex = 0\n",
    "    \n",
    "# Now, reconstruct the new table, we'll start by assembling a full list of all of the headers we want\n",
    "stored_header_info = []\n",
    "headers = ['Province']\n",
    "for subBlock in subBlocks:\n",
    "    for geoTemp in geoSpatial:\n",
    "        if(geoTemp[1] == subBlock[0]):\n",
    "            # These blocks are linked, grab the product name and year\n",
    "            fixName = subBlock[1].strip()\n",
    "            fixName = fixName.replace(\" \", \"-\")\n",
    "            fixName = fixName.replace(\",\", \"\")\n",
    "            fixName = fixName.replace(\" \", \"\")\n",
    "            hName = fixName + \"_\" + geoTemp[2]\n",
    "            # Store the header information for the table writing\n",
    "            if(not (hName in headers)):\n",
    "                headers.append(hName)\n",
    "                stored_header_info.append(tuple((fixName, geoTemp[0], subBlock[2], geoTemp[3], geoTemp[2])))\n",
    "            # Capture the value we want for saving and place it in a dictionary\n",
    "            vStr = str(sheet1[geoTemp[0]][1][subBlock[2]])\n",
    "            vStr = vStr.replace(\"$\", \",\")\n",
    "            dKey = (hName + '_' + geoTemp[3].replace(\" \", \"_\")).lower()\n",
    "            if(vStr):\n",
    "                saveDict[dKey] = vStr                \n",
    "            print(\"Saving: \" + dKey + \" => \" + vStr)\n",
    "outTable1 = pd.DataFrame(columns=headers)\n",
    "\n",
    "print(\"Writing New Table\")\n",
    "# Next, we're going to loop through each Province in our list and grab any connected data points\n",
    "tmp = []\n",
    "for province in ProvinceList:\n",
    "    tmp.append(province)\n",
    "    for header in stored_header_info:\n",
    "        keyTest = (header[0] + \"_\" + header[4] + \"_\" + province.replace(\" \", \"_\")).lower()\n",
    "        if(keyTest in saveDict):\n",
    "            tmp.append(saveDict[keyTest])        \n",
    "        else:\n",
    "            tmp.append(np.nan)\n",
    "    sObj = pd.Series(tmp, index=headers)\n",
    "    outTable1 = outTable1.append(sObj, ignore_index=True)\n",
    "    tmp = []\n",
    "# Write the output\n",
    "outTable1.to_csv(outFile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
